{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import csv\n",
    "from datetime import datetime, timezone\n",
    "from zoneinfo import ZoneInfo  # Use pytz if on Python <3.9\n",
    "\n",
    "# Set your parameters\n",
    "input_file = r\"D:\\reddit\\subreddits23\\wallstreetbets_comments\\wallstreetbets_comments\"\n",
    "output_file = r\"D:\\reddit\\output\\filtered_comments.csv\"\n",
    "from_date = datetime(2021, 1, 1, tzinfo=ZoneInfo(\"America/New_York\"))\n",
    "to_date = datetime(2023, 12, 31, 23, 59, 59, tzinfo=ZoneInfo(\"America/New_York\"))\n",
    "\n",
    "# Ensure the output directory exists\n",
    "output_dir = os.path.dirname(output_file)\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "\n",
    "def process_comments(input_file, output_file, from_date, to_date):\n",
    "    with open(input_file, 'r', encoding='utf-8') as file_in, open(output_file, 'w', encoding='utf-8', newline='') as file_out:\n",
    "        writer = csv.writer(file_out, quoting=csv.QUOTE_MINIMAL)\n",
    "        \n",
    "        # Writing CSV header for the comments dataset\n",
    "        writer.writerow([\"id\", \"score\", \"date\", \"time\", \"author\", \"parent_id\", \"link_id\", \"body\"])\n",
    "\n",
    "        for line in file_in:\n",
    "            try:\n",
    "                obj = json.loads(line.strip())  # Load each line as a JSON object\n",
    "                \n",
    "                # Convert created_utc to a timezone-aware datetime in UTC\n",
    "                created_utc = datetime.fromtimestamp(int(obj['created_utc']), tz=timezone.utc)\n",
    "                \n",
    "                # Convert UTC datetime to Eastern Time (ET)\n",
    "                created_et = created_utc.astimezone(ZoneInfo(\"America/New_York\"))\n",
    "\n",
    "                # Filter by date range in ET\n",
    "                if from_date <= created_et <= to_date:\n",
    "                    # Extract fields for the comments dataset\n",
    "                    comment_id = obj.get('id', '')\n",
    "                    score = obj.get('score', '')\n",
    "                    date = created_et.strftime(\"%Y-%m-%d\")\n",
    "                    time = created_et.strftime(\"%H:%M:%S\")  # Exact time in seconds\n",
    "                    author = obj.get('author', '')\n",
    "                    parent_id = obj.get('parent_id', '')\n",
    "                    # Remove the 't3_' prefix from link_id for later matching\n",
    "                    link_id = obj.get('link_id', '').replace('t3_', '')\n",
    "                    body = obj.get('body', '')\n",
    "\n",
    "                    # Write to CSV\n",
    "                    writer.writerow([comment_id, score, date, time, author, parent_id, link_id, body])\n",
    "\n",
    "            except json.JSONDecodeError as e:\n",
    "                print(f\"Skipping line due to JSON decode error: {e}\")\n",
    "\n",
    "# Run the processing function\n",
    "process_comments(input_file, output_file, from_date, to_date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data successfully split by year into separate files.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Define the file paths and create separate CSV writers for each year\n",
    "input_file = 'D:/reddit/output/filtered_comments.csv'\n",
    "output_files = {\n",
    "    2021: 'D:/reddit/output/filtered_comments_2021.csv',\n",
    "    2022: 'D:/reddit/output/filtered_comments_2022.csv',\n",
    "    2023: 'D:/reddit/output/filtered_comments_2023.csv'\n",
    "}\n",
    "\n",
    "# Initialize files for each year and write headers with full columns\n",
    "for year, file_path in output_files.items():\n",
    "    with open(file_path, 'w', encoding='utf-8', newline='') as file:\n",
    "        file.write(\"id,score,date,time,author,parent_id,link_id,body\\n\")  # Writing header\n",
    "\n",
    "# Process the input file in chunks\n",
    "chunk_size = 100000  # Adjust the chunk size based on available memory\n",
    "\n",
    "for chunk in pd.read_csv(input_file, chunksize=chunk_size, parse_dates=[\"date\"]):\n",
    "    # Drop rows with invalid dates (if any)\n",
    "    chunk = chunk.dropna(subset=['date'])\n",
    "\n",
    "    # Ensure columns are in the correct order to avoid misalignment\n",
    "    chunk = chunk[['id', 'score', 'date', 'time', 'author', 'parent_id', 'link_id', 'body']]\n",
    "\n",
    "    # Extract the year from the 'date' column\n",
    "    chunk['year'] = chunk['date'].dt.year\n",
    "\n",
    "    # Filter each chunk by year and append to the respective output file\n",
    "    for year in [2021, 2022, 2023]:\n",
    "        df_year = chunk[chunk['year'] == year]\n",
    "        if not df_year.empty:\n",
    "            with open(output_files[year], 'a', encoding='utf-8', newline='') as file:\n",
    "                df_year.to_csv(file, columns=['id', 'score', 'date', 'time', 'author', 'parent_id', 'link_id', 'body'], header=False, index=False)\n",
    "\n",
    "print(\"Data successfully split by year into separate files.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Path to the 2023 comments data\n",
    "file_path_2023 = 'D:/reddit/output/filtered_comments_2023.csv'\n",
    "\n",
    "# Load a sample of the 2023 data to verify it\n",
    "df_2023_sample = pd.read_csv(file_path_2023, nrows=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>score</th>\n",
       "      <th>date</th>\n",
       "      <th>time</th>\n",
       "      <th>author</th>\n",
       "      <th>parent_id</th>\n",
       "      <th>link_id</th>\n",
       "      <th>body</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>j2gxjta</td>\n",
       "      <td>4</td>\n",
       "      <td>2023-01-01</td>\n",
       "      <td>00:00:02</td>\n",
       "      <td>ninkorn</td>\n",
       "      <td>t3_zzbd4p</td>\n",
       "      <td>zzbd4p</td>\n",
       "      <td># HAPPY NEW YEAR FELLOW REGARDS\\n\\nDON'T CHANG...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>j2gxjv4</td>\n",
       "      <td>8</td>\n",
       "      <td>2023-01-01</td>\n",
       "      <td>00:00:02</td>\n",
       "      <td>TheLastOpus</td>\n",
       "      <td>t3_100bodo</td>\n",
       "      <td>100bodo</td>\n",
       "      <td>I would do the onlyfans route if i could.....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>j2gxjzq</td>\n",
       "      <td>1</td>\n",
       "      <td>2023-01-01</td>\n",
       "      <td>00:00:05</td>\n",
       "      <td>ABena2t</td>\n",
       "      <td>t1_j2gwm4l</td>\n",
       "      <td>zzlf9r</td>\n",
       "      <td>lol</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>j2gxk3d</td>\n",
       "      <td>2</td>\n",
       "      <td>2023-01-01</td>\n",
       "      <td>00:00:06</td>\n",
       "      <td>mathboom123</td>\n",
       "      <td>t1_j2gxfo0</td>\n",
       "      <td>zzbd4p</td>\n",
       "      <td>Me</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>j2gxk7i</td>\n",
       "      <td>1</td>\n",
       "      <td>2023-01-01</td>\n",
       "      <td>00:00:08</td>\n",
       "      <td>NarcolepticTreesnake</td>\n",
       "      <td>t3_100bujd</td>\n",
       "      <td>100bujd</td>\n",
       "      <td>Run from here as fast as you can and light $20...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>j2gy9ct</td>\n",
       "      <td>6</td>\n",
       "      <td>2023-01-01</td>\n",
       "      <td>00:07:05</td>\n",
       "      <td>ipfreelytv</td>\n",
       "      <td>t3_zzbd4p</td>\n",
       "      <td>zzbd4p</td>\n",
       "      <td>no losses this year hell ya</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>j2gy9qn</td>\n",
       "      <td>1</td>\n",
       "      <td>2023-01-01</td>\n",
       "      <td>00:07:11</td>\n",
       "      <td>VisualMod</td>\n",
       "      <td>t3_100ce61</td>\n",
       "      <td>100ce61</td>\n",
       "      <td>I am a bot from /r/wallstreetbets. Your submis...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>j2gya2n</td>\n",
       "      <td>0</td>\n",
       "      <td>2023-01-01</td>\n",
       "      <td>00:07:17</td>\n",
       "      <td>Liquid_Skeet</td>\n",
       "      <td>t3_zzbd4p</td>\n",
       "      <td>zzbd4p</td>\n",
       "      <td>Victoria Justice should make a sex tape with h...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>j2gyalo</td>\n",
       "      <td>1</td>\n",
       "      <td>2023-01-01</td>\n",
       "      <td>00:07:26</td>\n",
       "      <td>_Kenway</td>\n",
       "      <td>t3_zzbd4p</td>\n",
       "      <td>zzbd4p</td>\n",
       "      <td>working on a meme 2022 video recap, kinda late...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>j2gyaoh</td>\n",
       "      <td>2</td>\n",
       "      <td>2023-01-01</td>\n",
       "      <td>00:07:27</td>\n",
       "      <td>TJMBeav</td>\n",
       "      <td>t1_j2gy14n</td>\n",
       "      <td>zzbd4p</td>\n",
       "      <td>You as well. You have grown on me</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         id  score        date      time                author   parent_id  \\\n",
       "0   j2gxjta      4  2023-01-01  00:00:02               ninkorn   t3_zzbd4p   \n",
       "1   j2gxjv4      8  2023-01-01  00:00:02           TheLastOpus  t3_100bodo   \n",
       "2   j2gxjzq      1  2023-01-01  00:00:05               ABena2t  t1_j2gwm4l   \n",
       "3   j2gxk3d      2  2023-01-01  00:00:06           mathboom123  t1_j2gxfo0   \n",
       "4   j2gxk7i      1  2023-01-01  00:00:08  NarcolepticTreesnake  t3_100bujd   \n",
       "..      ...    ...         ...       ...                   ...         ...   \n",
       "95  j2gy9ct      6  2023-01-01  00:07:05            ipfreelytv   t3_zzbd4p   \n",
       "96  j2gy9qn      1  2023-01-01  00:07:11             VisualMod  t3_100ce61   \n",
       "97  j2gya2n      0  2023-01-01  00:07:17          Liquid_Skeet   t3_zzbd4p   \n",
       "98  j2gyalo      1  2023-01-01  00:07:26               _Kenway   t3_zzbd4p   \n",
       "99  j2gyaoh      2  2023-01-01  00:07:27               TJMBeav  t1_j2gy14n   \n",
       "\n",
       "    link_id                                               body  \n",
       "0    zzbd4p  # HAPPY NEW YEAR FELLOW REGARDS\\n\\nDON'T CHANG...  \n",
       "1   100bodo      I would do the onlyfans route if i could.....  \n",
       "2    zzlf9r                                                lol  \n",
       "3    zzbd4p                                                 Me  \n",
       "4   100bujd  Run from here as fast as you can and light $20...  \n",
       "..      ...                                                ...  \n",
       "95   zzbd4p                        no losses this year hell ya  \n",
       "96  100ce61  I am a bot from /r/wallstreetbets. Your submis...  \n",
       "97   zzbd4p  Victoria Justice should make a sex tape with h...  \n",
       "98   zzbd4p  working on a meme 2022 video recap, kinda late...  \n",
       "99   zzbd4p                  You as well. You have grown on me  \n",
       "\n",
       "[100 rows x 8 columns]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_2023_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del df_2023_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import csv\n",
    "from datetime import datetime, timezone\n",
    "from zoneinfo import ZoneInfo  # Use pytz if on Python <3.9\n",
    "\n",
    "# Set your parameters\n",
    "input_file = r\"D:\\reddit\\subreddits23\\wallstreetbets_submissions\\wallstreetbets_submissions\"\n",
    "output_file = r\"D:\\reddit\\output\\filtered_submissions.csv\"\n",
    "from_date = datetime(2021, 1, 1, tzinfo=ZoneInfo(\"America/New_York\"))\n",
    "to_date = datetime(2023, 12, 31, 23, 59, 59, tzinfo=ZoneInfo(\"America/New_York\"))\n",
    "\n",
    "# Ensure the output directory exists\n",
    "output_dir = os.path.dirname(output_file)\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "\n",
    "def process_file(input_file, output_file, from_date, to_date):\n",
    "    with open(input_file, 'r', encoding='utf-8') as file_in, open(output_file, 'w', encoding='utf-8', newline='') as file_out:\n",
    "        writer = csv.writer(file_out, quoting=csv.QUOTE_MINIMAL)\n",
    "        \n",
    "        # Writing CSV header based on required fields\n",
    "        writer.writerow([\"id\", \"score\", \"date\", \"time\", \"title\", \"author\", \"permalink\", \"selftext\", \"url\"])\n",
    "\n",
    "        for line in file_in:\n",
    "            try:\n",
    "                obj = json.loads(line.strip())  # Load each line as a JSON object\n",
    "                \n",
    "                # Convert created_utc to a timezone-aware datetime in UTC\n",
    "                created_utc = datetime.fromtimestamp(int(obj['created_utc']), tz=timezone.utc)\n",
    "                \n",
    "                # Convert UTC datetime to Eastern Time (ET)\n",
    "                created_et = created_utc.astimezone(ZoneInfo(\"America/New_York\"))\n",
    "\n",
    "                # Filter by date range in ET\n",
    "                if from_date <= created_et <= to_date:\n",
    "                    # Extract fields based on requirements\n",
    "                    submission_id = obj.get('id', '')\n",
    "                    score = obj.get('score', '')  # Assuming 'score' is available; adjust if needed\n",
    "                    date = created_et.strftime(\"%Y-%m-%d\")\n",
    "                    time = created_et.strftime(\"%H:%M:%S\")  # Exact time in seconds\n",
    "                    title = obj.get('title', '')\n",
    "                    author = obj.get('author', '')\n",
    "                    permalink = obj.get('permalink', '')\n",
    "                    selftext = obj.get('selftext', '')\n",
    "                    url = obj.get('url', '')\n",
    "\n",
    "                    # Write to CSV\n",
    "                    writer.writerow([submission_id, score, date, time, title, author, permalink, selftext, url])\n",
    "\n",
    "            except json.JSONDecodeError as e:\n",
    "                print(f\"Skipping line due to JSON decode error: {e}\")\n",
    "\n",
    "# Run the processing function\n",
    "process_file(input_file, output_file, from_date, to_date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submissions data successfully split by year into separate files.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Define the file paths and create separate CSV writers for each year\n",
    "input_file = 'D:/reddit/output/filtered_submissions.csv'\n",
    "output_files = {\n",
    "    2021: 'D:/reddit/output/filtered_submissions_2021.csv',\n",
    "    2022: 'D:/reddit/output/filtered_submissions_2022.csv',\n",
    "    2023: 'D:/reddit/output/filtered_submissions_2023.csv'\n",
    "}\n",
    "\n",
    "# Initialize files for each year and write headers with full columns\n",
    "for year, file_path in output_files.items():\n",
    "    with open(file_path, 'w', encoding='utf-8', newline='') as file:\n",
    "        file.write(\"id,score,date,time,title,author,permalink,selftext,url\\n\")  # Writing header\n",
    "\n",
    "# Process the input file in chunks\n",
    "chunk_size = 100000  # Adjust the chunk size based on available memory\n",
    "\n",
    "for chunk in pd.read_csv(input_file, chunksize=chunk_size, parse_dates=[\"date\"]):\n",
    "    # Drop rows with invalid dates (if any)\n",
    "    chunk = chunk.dropna(subset=['date'])\n",
    "\n",
    "    # Ensure columns are in the correct order to avoid misalignment\n",
    "    chunk = chunk[['id', 'score', 'date', 'time', 'title', 'author', 'permalink', 'selftext', 'url']]\n",
    "\n",
    "    # Extract the year from the 'date' column\n",
    "    chunk['year'] = chunk['date'].dt.year\n",
    "\n",
    "    # Filter each chunk by year and append to the respective output file\n",
    "    for year in [2021, 2022, 2023]:\n",
    "        df_year = chunk[chunk['year'] == year]\n",
    "        if not df_year.empty:\n",
    "            with open(output_files[year], 'a', encoding='utf-8', newline='') as file:\n",
    "                df_year.to_csv(file, columns=['id', 'score', 'date', 'time', 'title', 'author', 'permalink', 'selftext', 'url'], header=False, index=False)\n",
    "\n",
    "print(\"Submissions data successfully split by year into separate files.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Path to the 2023 comments data\n",
    "file_path_2023 = 'D:/reddit/output/filtered_submissions_2023.csv'\n",
    "\n",
    "# Load a sample of the 2023 data to verify it\n",
    "df_2023_sample = pd.read_csv(file_path_2023, nrows=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>score</th>\n",
       "      <th>date</th>\n",
       "      <th>time</th>\n",
       "      <th>title</th>\n",
       "      <th>author</th>\n",
       "      <th>permalink</th>\n",
       "      <th>selftext</th>\n",
       "      <th>url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>100cbuj</td>\n",
       "      <td>6</td>\n",
       "      <td>2023-01-01</td>\n",
       "      <td>00:02:48</td>\n",
       "      <td>Lazymeme.jpg</td>\n",
       "      <td>[deleted]</td>\n",
       "      <td>/r/wallstreetbets/comments/100cbuj/lazymemejpg/</td>\n",
       "      <td>[removed]</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>100ce61</td>\n",
       "      <td>1</td>\n",
       "      <td>2023-01-01</td>\n",
       "      <td>00:07:08</td>\n",
       "      <td>Brainchip Stock To $2??</td>\n",
       "      <td>Firm-Blueberry-5919</td>\n",
       "      <td>/r/wallstreetbets/comments/100ce61/brainchip_s...</td>\n",
       "      <td>[removed]</td>\n",
       "      <td>https://www.reddit.com/r/wallstreetbets/commen...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>100ceov</td>\n",
       "      <td>12</td>\n",
       "      <td>2023-01-01</td>\n",
       "      <td>00:08:03</td>\n",
       "      <td>2023 here we come, hope it turns out better th...</td>\n",
       "      <td>[deleted]</td>\n",
       "      <td>/r/wallstreetbets/comments/100ceov/2023_here_w...</td>\n",
       "      <td>[removed]</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>100cf5c</td>\n",
       "      <td>1</td>\n",
       "      <td>2023-01-01</td>\n",
       "      <td>00:08:56</td>\n",
       "      <td>Looks like OSU discovered options...</td>\n",
       "      <td>[deleted]</td>\n",
       "      <td>/r/wallstreetbets/comments/100cf5c/looks_like_...</td>\n",
       "      <td>[removed]</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>100ch63</td>\n",
       "      <td>108</td>\n",
       "      <td>2023-01-01</td>\n",
       "      <td>00:12:28</td>\n",
       "      <td>Doesn't feel right if I don't post this here e...</td>\n",
       "      <td>SynapseCero</td>\n",
       "      <td>/r/wallstreetbets/comments/100ch63/doesnt_feel...</td>\n",
       "      <td>Well, end of the year, how could I not post th...</td>\n",
       "      <td>https://www.reddit.com/r/wallstreetbets/commen...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>100musf</td>\n",
       "      <td>1</td>\n",
       "      <td>2023-01-01</td>\n",
       "      <td>11:14:52</td>\n",
       "      <td>More AI Art: Jerome Powell Eating Barbecue Bul...</td>\n",
       "      <td>[deleted]</td>\n",
       "      <td>/r/wallstreetbets/comments/100musf/more_ai_art...</td>\n",
       "      <td>[removed]</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>100mut5</td>\n",
       "      <td>0</td>\n",
       "      <td>2023-01-01</td>\n",
       "      <td>11:14:54</td>\n",
       "      <td>Where to paper trade options?</td>\n",
       "      <td>saippuakauppias</td>\n",
       "      <td>/r/wallstreetbets/comments/100mut5/where_to_pa...</td>\n",
       "      <td>[removed]</td>\n",
       "      <td>https://www.reddit.com/r/wallstreetbets/commen...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>100n8rj</td>\n",
       "      <td>1</td>\n",
       "      <td>2023-01-01</td>\n",
       "      <td>11:33:36</td>\n",
       "      <td>I’ll make it back this year…</td>\n",
       "      <td>[deleted]</td>\n",
       "      <td>/r/wallstreetbets/comments/100n8rj/ill_make_it...</td>\n",
       "      <td>[removed]</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>100na2e</td>\n",
       "      <td>282</td>\n",
       "      <td>2023-01-01</td>\n",
       "      <td>11:35:19</td>\n",
       "      <td>Hoping these calls will bring me back</td>\n",
       "      <td>[deleted]</td>\n",
       "      <td>/r/wallstreetbets/comments/100na2e/hoping_thes...</td>\n",
       "      <td>[deleted]</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>100na7q</td>\n",
       "      <td>1</td>\n",
       "      <td>2023-01-01</td>\n",
       "      <td>11:35:31</td>\n",
       "      <td>Screw You, ‘22</td>\n",
       "      <td>[deleted]</td>\n",
       "      <td>/r/wallstreetbets/comments/100na7q/screw_you_22/</td>\n",
       "      <td>[removed]</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         id  score        date      time  \\\n",
       "0   100cbuj      6  2023-01-01  00:02:48   \n",
       "1   100ce61      1  2023-01-01  00:07:08   \n",
       "2   100ceov     12  2023-01-01  00:08:03   \n",
       "3   100cf5c      1  2023-01-01  00:08:56   \n",
       "4   100ch63    108  2023-01-01  00:12:28   \n",
       "..      ...    ...         ...       ...   \n",
       "95  100musf      1  2023-01-01  11:14:52   \n",
       "96  100mut5      0  2023-01-01  11:14:54   \n",
       "97  100n8rj      1  2023-01-01  11:33:36   \n",
       "98  100na2e    282  2023-01-01  11:35:19   \n",
       "99  100na7q      1  2023-01-01  11:35:31   \n",
       "\n",
       "                                                title               author  \\\n",
       "0                                        Lazymeme.jpg            [deleted]   \n",
       "1                             Brainchip Stock To $2??  Firm-Blueberry-5919   \n",
       "2   2023 here we come, hope it turns out better th...            [deleted]   \n",
       "3                Looks like OSU discovered options...            [deleted]   \n",
       "4   Doesn't feel right if I don't post this here e...          SynapseCero   \n",
       "..                                                ...                  ...   \n",
       "95  More AI Art: Jerome Powell Eating Barbecue Bul...            [deleted]   \n",
       "96                      Where to paper trade options?      saippuakauppias   \n",
       "97                       I’ll make it back this year…            [deleted]   \n",
       "98              Hoping these calls will bring me back            [deleted]   \n",
       "99                                     Screw You, ‘22            [deleted]   \n",
       "\n",
       "                                            permalink  \\\n",
       "0     /r/wallstreetbets/comments/100cbuj/lazymemejpg/   \n",
       "1   /r/wallstreetbets/comments/100ce61/brainchip_s...   \n",
       "2   /r/wallstreetbets/comments/100ceov/2023_here_w...   \n",
       "3   /r/wallstreetbets/comments/100cf5c/looks_like_...   \n",
       "4   /r/wallstreetbets/comments/100ch63/doesnt_feel...   \n",
       "..                                                ...   \n",
       "95  /r/wallstreetbets/comments/100musf/more_ai_art...   \n",
       "96  /r/wallstreetbets/comments/100mut5/where_to_pa...   \n",
       "97  /r/wallstreetbets/comments/100n8rj/ill_make_it...   \n",
       "98  /r/wallstreetbets/comments/100na2e/hoping_thes...   \n",
       "99   /r/wallstreetbets/comments/100na7q/screw_you_22/   \n",
       "\n",
       "                                             selftext  \\\n",
       "0                                           [removed]   \n",
       "1                                           [removed]   \n",
       "2                                           [removed]   \n",
       "3                                           [removed]   \n",
       "4   Well, end of the year, how could I not post th...   \n",
       "..                                                ...   \n",
       "95                                          [removed]   \n",
       "96                                          [removed]   \n",
       "97                                          [removed]   \n",
       "98                                          [deleted]   \n",
       "99                                          [removed]   \n",
       "\n",
       "                                                  url  \n",
       "0                                                 NaN  \n",
       "1   https://www.reddit.com/r/wallstreetbets/commen...  \n",
       "2                                                 NaN  \n",
       "3                                                 NaN  \n",
       "4   https://www.reddit.com/r/wallstreetbets/commen...  \n",
       "..                                                ...  \n",
       "95                                                NaN  \n",
       "96  https://www.reddit.com/r/wallstreetbets/commen...  \n",
       "97                                                NaN  \n",
       "98                                                NaN  \n",
       "99                                                NaN  \n",
       "\n",
       "[100 rows x 9 columns]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_2023_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ASUS\\AppData\\Local\\Temp\\ipykernel_67176\\438859393.py:58: DtypeWarning: Columns (1) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  comments_df = pd.read_csv(comments_files[year], on_bad_lines='skip')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dropped 9 rows with NaN parent_id for year 2021\n",
      "Unmatched comments for 2021 saved to D:/reddit/output/unmatched_comments_2021.csv\n",
      "Flattened data for 2021 saved to D:/reddit/output/flattened_submissions_with_comments_2021.csv\n",
      "Dropped 0 rows with NaN parent_id for year 2022\n",
      "Unmatched comments for 2022 saved to D:/reddit/output/unmatched_comments_2022.csv\n",
      "Flattened data for 2022 saved to D:/reddit/output/flattened_submissions_with_comments_2022.csv\n",
      "Dropped 0 rows with NaN parent_id for year 2023\n",
      "Unmatched comments for 2023 saved to D:/reddit/output/unmatched_comments_2023.csv\n",
      "Flattened data for 2023 saved to D:/reddit/output/flattened_submissions_with_comments_2023.csv\n",
      "No errors encountered during processing.\n",
      "Drop log saved to D:/reddit/output/drop_log.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import csv\n",
    "\n",
    "# Define file paths for submissions and comments by year\n",
    "submissions_files = {\n",
    "    2021: 'D:/reddit/output/filtered_submissions_2021.csv',\n",
    "    2022: 'D:/reddit/output/filtered_submissions_2022.csv',\n",
    "    2023: 'D:/reddit/output/filtered_submissions_2023.csv'\n",
    "}\n",
    "comments_files = {\n",
    "    2021: 'D:/reddit/output/filtered_comments_2021.csv',\n",
    "    2022: 'D:/reddit/output/filtered_comments_2022.csv',\n",
    "    2023: 'D:/reddit/output/filtered_comments_2023.csv'\n",
    "}\n",
    "\n",
    "# Output files for merged data and unmatched comments\n",
    "output_files = {\n",
    "    2021: 'D:/reddit/output/flattened_submissions_with_comments_2021.csv',\n",
    "    2022: 'D:/reddit/output/flattened_submissions_with_comments_2022.csv',\n",
    "    2023: 'D:/reddit/output/flattened_submissions_with_comments_2023.csv'\n",
    "}\n",
    "unmatched_files = {\n",
    "    2021: 'D:/reddit/output/unmatched_comments_2021.csv',\n",
    "    2022: 'D:/reddit/output/unmatched_comments_2022.csv',\n",
    "    2023: 'D:/reddit/output/unmatched_comments_2023.csv'\n",
    "}\n",
    "error_log_file = 'D:/reddit/output/error_log.csv'  # Error log file to keep track of problematic rows\n",
    "\n",
    "# Initialize a list to keep track of errors and dropped rows\n",
    "error_log = []\n",
    "drop_log = []\n",
    "\n",
    "# Process each year separately\n",
    "for year in [2021, 2022, 2023]:\n",
    "    # Load submissions with error handling\n",
    "    try:\n",
    "        submissions_df = pd.read_csv(submissions_files[year], on_bad_lines='skip')\n",
    "    except pd.errors.ParserError as e:\n",
    "        error_log.append({'file': submissions_files[year], 'year': year, 'error': str(e)})\n",
    "        print(f\"Error reading submissions file for {year}: {e}\")\n",
    "        continue\n",
    "\n",
    "    # Rename columns in submissions to differentiate them when merged with comments\n",
    "    submissions_df = submissions_df.rename(columns={\n",
    "        \"id\": \"submission_id\",\n",
    "        \"score\": \"submission_score\",\n",
    "        \"date\": \"submission_date\",\n",
    "        \"time\": \"submission_time\",\n",
    "        \"title\": \"submission_title\",\n",
    "        \"author\": \"submission_author\",\n",
    "        \"permalink\": \"submission_permalink\",\n",
    "        \"selftext\": \"submission_selftext\",\n",
    "        \"url\": \"submission_url\"\n",
    "    })\n",
    "\n",
    "    # Load comments with error handling\n",
    "    try:\n",
    "        comments_df = pd.read_csv(comments_files[year], on_bad_lines='skip')\n",
    "    except pd.errors.ParserError as e:\n",
    "        error_log.append({'file': comments_files[year], 'year': year, 'error': str(e)})\n",
    "        print(f\"Error reading comments file for {year}: {e}\")\n",
    "        continue\n",
    "\n",
    "    # Count the original number of rows in comments\n",
    "    original_row_count = len(comments_df)\n",
    "\n",
    "    # Remove rows with NaN values in `parent_id`\n",
    "    comments_df = comments_df.dropna(subset=['parent_id'])\n",
    "\n",
    "    # Count and log the number of dropped rows\n",
    "    dropped_row_count = original_row_count - len(comments_df)\n",
    "    drop_log.append({'year': year, 'dropped_rows': dropped_row_count})\n",
    "    print(f\"Dropped {dropped_row_count} rows with NaN parent_id for year {year}\")\n",
    "\n",
    "    # Filter comments to include only those with `parent_id` pointing to a submission (i.e., starts with \"t3_\")\n",
    "    comments_df = comments_df[comments_df['parent_id'].str.startswith('t3_')].copy()\n",
    "\n",
    "    # Clean up `parent_id` by removing the \"t3_\" prefix\n",
    "    comments_df['parent_id'] = comments_df['parent_id'].str.replace('t3_', '')\n",
    "\n",
    "    # Rename columns in comments for clarity\n",
    "    comments_df = comments_df.rename(columns={\n",
    "        \"id\": \"comment_id\",\n",
    "        \"score\": \"comment_score\",\n",
    "        \"date\": \"comment_date\",\n",
    "        \"time\": \"comment_time\",\n",
    "        \"author\": \"comment_author\",\n",
    "        \"body\": \"comment_body\"\n",
    "    })\n",
    "\n",
    "    # Merge comments with their associated submission data (left join to keep all comments)\n",
    "    merged_df = comments_df.merge(submissions_df, left_on='parent_id', right_on='submission_id', how='left')\n",
    "\n",
    "    # Separate unmatched comments (those with NaN in `submission_id` column)\n",
    "    unmatched_comments_df = merged_df[merged_df['submission_id'].isna()]\n",
    "\n",
    "    # Log unmatched comments to a separate CSV file\n",
    "    if not unmatched_comments_df.empty:\n",
    "        unmatched_comments_df.to_csv(unmatched_files[year], index=False)\n",
    "        print(f\"Unmatched comments for {year} saved to {unmatched_files[year]}\")\n",
    "    else:\n",
    "        print(f\"No unmatched comments for {year}.\")\n",
    "\n",
    "    # Filter out unmatched comments from the merged data (optional)\n",
    "    matched_comments_df = merged_df.dropna(subset=['submission_id'])\n",
    "\n",
    "    # Save the matched data to a flat CSV file\n",
    "    matched_comments_df.to_csv(output_files[year], index=False)\n",
    "    print(f\"Flattened data for {year} saved to {output_files[year]}\")\n",
    "\n",
    "# Save the error log to a CSV file if there were any errors\n",
    "if error_log:\n",
    "    pd.DataFrame(error_log).to_csv(error_log_file, index=False)\n",
    "    print(f\"Error log saved to {error_log_file}\")\n",
    "else:\n",
    "    print(\"No errors encountered during processing.\")\n",
    "\n",
    "# Save the drop log to a CSV file to record the dropped rows\n",
    "drop_log_file = 'D:/reddit/output/drop_log.csv'\n",
    "pd.DataFrame(drop_log).to_csv(drop_log_file, index=False)\n",
    "print(f\"Drop log saved to {drop_log_file}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
